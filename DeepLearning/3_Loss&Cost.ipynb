{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Loss&Cost.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## <center> **ACTIVATION FUNCTIONS** </center>\n",
        "\n",
        "***Sigmoid***: changes into range 0 to 1 <br>\n",
        "***Tanh:*** changes into range -1 to 1\n",
        "\n",
        "* **NOTE/TIP: USE SIGMOID IN OUTPUT LAYER AND tanh IN ALL THE OTHER LAYERS.**\n",
        "<br><br>\n",
        "\n",
        "**ISSUES WITH TANH AND SIGMOID**: <br>\n",
        "VANISHING GRADIENT PROBLEM-> which makes calculation very slow\n",
        "\n",
        "TO SOLVE THIS PROBLEM A BIT, *ReLU is used* <br>\n",
        "***ReLU***: gives max(0,x)\n",
        "\n",
        "* **NOTE/TIP: If you're not sure what activation function to use in HIDDEN LAYERS, just use ReLU**\n",
        "\n",
        "**ReLU still has vanishing gradient problem <br>\n",
        "To overcome that, LEAKY RELU IS USED.**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ix5RyI-NG3TN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center> **Gradient Descent**\n",
        "It is an iterative optimization algorithm used in machine learning to find the best results (minima of a curve).\n",
        "\n",
        "- Gradient means the rate of inclination or declination of a slope.\n",
        "\n",
        "- Descent means the instance of descending.\n",
        "---"
      ],
      "metadata": {
        "id": "ojYt58CR19J_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center> **LOSS AND COST FUNCTION** </center> \n",
        "***Loss function:*** when we refer to the error for a single training example.\n",
        "\n",
        "***Cost function:*** Used to refer to an average of all the loss functions over an entire training dataset. \n",
        "- MEAN ABSOLUTE ERROR\n",
        "- MEAN SQUARED ERROR\n",
        "- **For logistic regression, we use LOG LOSS or BINARY CROSS ENTROPY**\n",
        "---"
      ],
      "metadata": {
        "id": "FwhqwlAVzXuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##  <CENTER> <B> EPOCH </B> \n",
        "<B> \n",
        "- An epoch means training the neural network with all the training data for one cycle.\n",
        "- (finishing all the samples in a round)\n",
        "- In an epoch, we use all of the data exactly once. A forward pass and a backward pass together are counted as one pass.\n",
        "- With each epoch, the dataset’s internal model parameters are updated\n",
        "- Only one epoch leads to underfitting of the curve in the graph.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nkV6SpTz0GsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center> **BATCH**<br>\n",
        "\n",
        "**We can’t pass the entire dataset into the neural net at once. So, we divide dataset into Number of Batches.**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dFozWb_03L9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **FORWARD AND BACKWARD PASS**\n",
        "\n",
        "1. The **\"forward pass\"** refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer.\n",
        "\n",
        "A loss function is calculated from the output values.\n",
        "\n",
        "2. And then **\"backward pass\"** refers to process of counting changes in weights, using gradient descent algorithm (or similar). Computation is made from last layer, backward to the first layer.\n",
        "\n",
        "Backward and forward pass makes together one \"iteration\"\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "okL0soYh285B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNDERSTANDING LOSS FUNCTIONS"
      ],
      "metadata": {
        "id": "lvHKxIjtF2TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_Predicted = np.array([2,7,4,1,2,0])\n",
        "y_True = np.array([2.2,6.5,4,0,0.80,0])"
      ],
      "metadata": {
        "id": "9iRG5JtJHfY4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean absolute error\n",
        "np.mean(np.abs(y_Predicted- y_True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI7iNoaDBACy",
        "outputId": "d867568a-e6a7-4ccd-abd4-cf00f1f354c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4833333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mean squared error\n",
        "np.mean(np.abs(y_Predicted**2 - y_True**2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRIvrIhcFRRw",
        "outputId": "8f736117-bdc2-4aa0-bded-cf43ca6c2ea9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9916666666666665"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LOG LOSS FUNCTION/BINARY CROSS ENTROPY**\n",
        "\n",
        "Since log(0) is not defined, we have to change the value 0 to something which is very close to zero (but not exactly zero. for example, 0.000000000001)"
      ],
      "metadata": {
        "id": "zpMAI2HwGfzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOG LOSS FUNCTIONS\n",
        "\n",
        "#create a variable very close to 0\n",
        "epsilon= 1e-15\n",
        "\n",
        "Y_Predicted_new = [max(i,epsilon) for i in y_Predicted]\n",
        "Y_Predicted_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-uD4SDoF6Vk",
        "outputId": "e0660ed8-17ee-41e8-a651-447b3e77a5b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 7, 4, 1, 2, 1e-15]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The value at arr[5] has changed**"
      ],
      "metadata": {
        "id": "YuJcSFzWHJh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# similary, change the value of 1 to a number very close to 1, but not 1.\n",
        "Y_Predicted_new = [max(i,1-epsilon) for i in Y_Predicted_new]\n",
        "Y_Predicted_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMk0XT8vHIIE",
        "outputId": "95aa61bd-dfb1-41ed-b4bb-22d9e323dc86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 7, 4, 1, 2, 0.999999999999999]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "-(np.mean(y_True*np.log(Y_Predicted_new)+(1-y_True)*np.log(1-Y_Predicted_new)))"
      ],
      "metadata": {
        "id": "gZaWWRZ-KMZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FUNCTION TO CALCULATE LOSS "
      ],
      "metadata": {
        "id": "lG6ylXYbJ1h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true, y_predicted):\n",
        "    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n",
        "    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n",
        "    y_predicted_new = np.array(y_predicted_new)\n",
        "    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))"
      ],
      "metadata": {
        "id": "E0HVG_lmJgeR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_loss(y_True,y_Predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNMkuVwqJhmH",
        "outputId": "ef023f93-69ab-4a07-d5b8-1f77db267ecd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-48.93106598914958"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}